{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecOJBg1i2Gt-",
        "outputId": "6c783c6d-5578-4811-c5e6-cd2347f3c091"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zn1Zc_N10NeK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoConfig\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_chkpt = 'bert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_chkpt)\n",
        "config = AutoConfig.from_pretrained(model_chkpt)"
      ],
      "metadata": {
        "id": "PBi9f1Vy1mUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'My life is wonderful because I get to train Deep Learning models every day.'"
      ],
      "metadata": {
        "id": "FBL-Kx_z2TGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer(text, return_tensors='pt', add_special_tokens=False)['input_ids']\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEKFHhAf2evB",
        "outputId": "38c90291-d52a-4ac4-ca08-94721290f899"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2026, 2166, 2003, 6919, 2138, 1045, 2131, 2000, 3345, 2784, 4083, 4275,\n",
              "         2296, 2154, 1012]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'O número máximo de tokens é: {config.vocab_size}')\n",
        "print(f'A dimensão do embedding é: D={config.hidden_size}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfuzsHBe4OdH",
        "outputId": "012ff5dd-daa7-43b3-9b5d-ac4260c1c039"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O número máximo de tokens é: 30522\n",
            "A dimensão do embedding é: D=768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = nn.Embedding(config.vocab_size, config.hidden_size)\n",
        "embeddings = embedding_layer(tokens)\n",
        "print(f'O formato dos tokens de entrada é: {tokens.shape}')\n",
        "print(f'O formato dos embeddings é: {embeddings.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mN3iKQDM3zv3",
        "outputId": "2d868e1a-57ef-45a8-ba5b-0581112abb77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O formato dos tokens de entrada é: torch.Size([1, 15])\n",
            "O formato dos embeddings é: torch.Size([1, 15, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scale_dot_product_attention(query, key, value):\n",
        "    # número de colunas da matriz K\n",
        "    M_k = key.size(-1)\n",
        "    # determina as energias\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(M_k)\n",
        "    # determina os pesos do alinhamento\n",
        "    attention_weights = F.softmax(scores, dim=-1)\n",
        "    # multiplica pesos pela matriz V\n",
        "    return torch.matmul(attention_weights, value)\n",
        "\n",
        "class AttentionHead(nn.Module):\n",
        "    def __init__(self, embed_dim, head_dim):\n",
        "        super().__init__()\n",
        "        # matriz W^(Q)\n",
        "        self.q = nn.Linear(embed_dim, head_dim)\n",
        "        # matriz W^(K)\n",
        "        self.k = nn.Linear(embed_dim, head_dim)\n",
        "        # matriz W^(V)\n",
        "        self.v = nn.Linear(embed_dim, head_dim)\n",
        "\n",
        "    def forward(self, hidden_state):\n",
        "        attn_outputs = scale_dot_product_attention(self.q(hidden_state),\n",
        "                                                   self.k(hidden_state),\n",
        "                                                   self.v(hidden_state))\n",
        "        return attn_outputs\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        # dimensão do embedding\n",
        "        embed_dim = config.hidden_size\n",
        "        # número de cabeças\n",
        "        num_heads = config.num_attention_heads\n",
        "        # dimensão de cada cabeça\n",
        "        head_dim = embed_dim // num_heads\n",
        "        self.heads = nn.ModuleList(\n",
        "            [AttentionHead(embed_dim, head_dim) for _ in range(num_heads)]\n",
        "        )\n",
        "        self.output_linear = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, hidden_state):\n",
        "        x = torch.cat([h(hidden_state) for h in self.heads], dim=-1)\n",
        "        x = self.output_linear(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "ygWdM7x955zz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multihead_attn = MultiHeadAttention(config)\n",
        "attn_outputs = multihead_attn(embeddings)\n",
        "print(f'O formato da saída da camada MHA é: {attn_outputs.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVzbvF8z-tO0",
        "outputId": "76d7e5e0-02a2-44fc-e7f2-62fc326e18a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O formato da saída da camada MHA é: torch.Size([1, 15, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        # camada 1\n",
        "        self.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        # camada 2\n",
        "        self.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        # função de ativação\n",
        "        self.gelu = nn.GELU()\n",
        "        # dropout\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear_1(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.linear_2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "9yROj-Fr_zpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feed_forward = FeedForward(config)\n",
        "ff_outputs = feed_forward(attn_outputs)\n",
        "print(f'O formato da saída da camada FF é: {ff_outputs.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dW4RTlwZArB2",
        "outputId": "9cb8e429-7f68-4cce-8671-1b296cec2b16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O formato da saída da camada FF é: torch.Size([1, 15, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LLMEncoderLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        # layer norm 1\n",
        "        self.layer_norm_1 = nn.LayerNorm(config.hidden_size)\n",
        "        # layer norm 2\n",
        "        self.layer_norm_2 = nn.LayerNorm(config.hidden_size)\n",
        "        # MMA\n",
        "        self.attention = MultiHeadAttention(config)\n",
        "        # rede FF\n",
        "        self.feed_forward = FeedForward(config)\n",
        "\n",
        "    def forward(self, hidden_state):\n",
        "        # passa o hidden state pela camada de normalização\n",
        "        x = self.layer_norm_1(hidden_state)\n",
        "        # passa o hidden state pelo MHA\n",
        "        multihead_att_output = self.attention(x)\n",
        "        # soma com a própria entrada (skip connection)\n",
        "        x = x + multihead_att_output\n",
        "        # passa resultado pela camada de normalização 2\n",
        "        x = self.layer_norm_2(x)\n",
        "        # passa resultado pela camada FF\n",
        "        ff_output = self.feed_forward(x)\n",
        "        # soma com a própria entrada (skip connection)\n",
        "        x = x + ff_output\n",
        "        return x"
      ],
      "metadata": {
        "id": "LWo32SzwBBmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_layer = LLMEncoderLayer(config)\n",
        "encoder_outputs = encoder_layer(embeddings)\n",
        "print(f'O formato da saída do encoder é: {encoder_outputs.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yu9AVst-CSTE",
        "outputId": "4015c0cf-eef5-47ff-a3cb-91bd89f863ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O formato da saída do encoder é: torch.Size([1, 15, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        # embedding layer\n",
        "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
        "        # camada de embedding\n",
        "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "        # normalização\n",
        "        self.layer_norm = nn.LayerNorm(config.hidden_size)\n",
        "        # camada dropout\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        # passa tokens pela camada de embedding\n",
        "        token_embeddings = self.word_embeddings(tokens)\n",
        "        # obtém positional embeddings\n",
        "        position_ids = torch.arange(tokens.size(-1), dtype=torch.long).unsqueeze(0)\n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        # soma embeddings\n",
        "        embeddings = token_embeddings + position_embeddings\n",
        "        # passa pela camada de normalização\n",
        "        embeddings = self.layer_norm(embeddings)\n",
        "        # passa pela camada de dropout\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        # retorna embeddings\n",
        "        return embeddings"
      ],
      "metadata": {
        "id": "POA60ZN3CnrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = Embeddings(config)\n",
        "embeddings = embedding_layer(tokens)\n",
        "print(f'O formato dos embeddings é: {embeddings.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsEv8z6EExES",
        "outputId": "26e12e86-d00e-4340-a76f-4a6421930d68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O formato dos embeddings é: torch.Size([1, 15, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LLMEncoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        # camadas de embedding\n",
        "        self.embeddings = Embeddings(config)\n",
        "        # lista de encoder layers\n",
        "        self.layers = nn.ModuleList([LLMEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        # passa tokens pela camada de embedding\n",
        "        x = self.embeddings(tokens)\n",
        "        # passa resultado pela rede de codificação\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "jWDCA1fqGE94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = LLMEncoder(config)\n",
        "encoder_outputs = encoder(tokens)\n",
        "print(f'O formato da saída do encoder é: {encoder_outputs.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uf4rzZ5rHc4X",
        "outputId": "dcc78da1-96f2-4d93-e73e-53a8657b6a8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O formato da saída do encoder é: torch.Size([1, 15, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scale_dot_product_attention(query, key, value, mask=None):\n",
        "    # número de colunas da matriz K\n",
        "    M_k = key.size(-1)\n",
        "    # determina as energias\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(M_k)\n",
        "    # mascaramentos\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    # determina os pesos do alinhamento\n",
        "    attention_weights = F.softmax(scores, dim=-1)\n",
        "    # multiplica pesos pela matriz V\n",
        "    return torch.matmul(attention_weights, value)"
      ],
      "metadata": {
        "id": "kPyMUN_TIkiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = torch.randn(15, 15)\n",
        "scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_rZos01JT0y",
        "outputId": "02f113cc-5097-4a45-8231-a11189cdcbcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.8417,  0.8060,  1.2483, -0.6898,  0.0068,  1.3136,  0.1675,  0.9666,\n",
              "         -0.2039,  0.0974,  1.2938, -0.1983, -0.0227, -1.2601,  1.0182],\n",
              "        [-0.5542, -1.7567,  0.5921, -1.0961, -0.1355,  0.2571, -0.5533, -1.4093,\n",
              "          0.4308,  0.4744, -0.5886, -0.6547, -0.5521, -0.0437, -0.6133],\n",
              "        [ 0.0748,  0.3582,  0.3468, -1.5498,  0.5766,  0.1037, -0.7184, -0.4379,\n",
              "          0.9649, -1.2862, -0.5807,  1.3402, -1.5921,  0.6391, -0.2642],\n",
              "        [-1.8382,  0.3454,  1.2742,  0.1523,  0.5679,  1.5151, -0.6248, -1.3665,\n",
              "         -1.4625, -0.2983,  0.4415, -2.2831,  0.6963, -1.7661,  0.3628],\n",
              "        [-1.0595,  0.9681, -1.5611,  0.4924,  0.8435,  1.5706, -1.6356, -0.3670,\n",
              "          0.8104,  0.9904,  0.2503,  1.0078,  0.4570, -0.5926, -1.6973],\n",
              "        [-0.2042, -0.0841,  0.0833,  1.0285, -0.4692, -1.6611,  0.5532,  0.8822,\n",
              "         -0.5486,  1.3558,  0.4309,  1.5466,  1.1486,  1.2052, -1.5796],\n",
              "        [-0.4567, -0.1212,  1.3571, -1.6048,  0.1859,  1.4100, -0.1368, -0.4528,\n",
              "          0.3568, -1.0783,  0.3209,  0.0648, -1.3334, -0.3782, -0.1943],\n",
              "        [ 1.8453,  1.2512, -1.4420,  2.2701,  0.1324, -0.6747, -0.4882, -1.4060,\n",
              "          0.0118, -0.2073,  0.5448, -0.2676,  0.6980, -1.8168, -0.1142],\n",
              "        [ 0.9325, -2.4414,  1.5666,  1.5675,  1.0049, -1.2909,  0.1821, -0.6779,\n",
              "          1.6750,  0.7906, -0.4095,  1.4037, -0.0192, -1.3568,  0.1729],\n",
              "        [-0.2466,  0.6949, -0.9060, -2.0312, -1.4191, -1.6335,  1.5888,  0.5778,\n",
              "          0.4629, -1.2370, -0.3264, -0.7861,  1.1555,  0.8566, -1.5505],\n",
              "        [-1.6077,  1.6273,  0.1035,  0.6953, -0.6378,  1.2458, -1.0265, -0.9100,\n",
              "         -0.6797, -0.0780, -1.1265, -0.8579,  0.0318, -0.4184, -1.7164],\n",
              "        [ 1.5427, -0.6321,  1.2587,  1.2481,  0.4806,  0.5224,  0.9189,  0.3613,\n",
              "         -0.3084,  0.1027,  0.1049, -0.4906,  1.3910, -1.0518, -1.4716],\n",
              "        [-0.9186,  0.1403,  0.4370, -2.0116,  0.4895, -1.2517,  1.2704,  0.3608,\n",
              "          0.6040,  1.9112, -0.3340,  0.1679,  0.3911,  1.5998, -2.2511],\n",
              "        [-0.9563, -0.5518, -0.0837,  0.4482,  0.9669, -0.7583, -1.1009, -0.1881,\n",
              "          1.0936, -0.2113,  0.1203,  0.9837, -0.4094, -0.0919, -0.2103],\n",
              "        [ 0.5489,  1.1825,  1.7836, -0.7314, -0.7873,  0.6787,  0.8364,  0.4116,\n",
              "         -2.1631,  0.5310,  1.1025, -1.5901, -0.2338, -1.6032,  0.7842]])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask = torch.tril(torch.ones(15, 15))\n",
        "scores = scores.masked_fill(mask == 0, -float('inf'))\n",
        "scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTMxfcQMKKUU",
        "outputId": "619196c4-0c3b-48d2-cfb8-a479d71db40f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.8417,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
              "        [-0.5542, -1.7567,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
              "        [ 0.0748,  0.3582,  0.3468,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
              "        [-1.8382,  0.3454,  1.2742,  0.1523,    -inf,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
              "        [-1.0595,  0.9681, -1.5611,  0.4924,  0.8435,    -inf,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
              "        [-0.2042, -0.0841,  0.0833,  1.0285, -0.4692, -1.6611,    -inf,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
              "        [-0.4567, -0.1212,  1.3571, -1.6048,  0.1859,  1.4100, -0.1368,    -inf,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
              "        [ 1.8453,  1.2512, -1.4420,  2.2701,  0.1324, -0.6747, -0.4882, -1.4060,\n",
              "            -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
              "        [ 0.9325, -2.4414,  1.5666,  1.5675,  1.0049, -1.2909,  0.1821, -0.6779,\n",
              "          1.6750,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
              "        [-0.2466,  0.6949, -0.9060, -2.0312, -1.4191, -1.6335,  1.5888,  0.5778,\n",
              "          0.4629, -1.2370,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
              "        [-1.6077,  1.6273,  0.1035,  0.6953, -0.6378,  1.2458, -1.0265, -0.9100,\n",
              "         -0.6797, -0.0780, -1.1265,    -inf,    -inf,    -inf,    -inf],\n",
              "        [ 1.5427, -0.6321,  1.2587,  1.2481,  0.4806,  0.5224,  0.9189,  0.3613,\n",
              "         -0.3084,  0.1027,  0.1049, -0.4906,    -inf,    -inf,    -inf],\n",
              "        [-0.9186,  0.1403,  0.4370, -2.0116,  0.4895, -1.2517,  1.2704,  0.3608,\n",
              "          0.6040,  1.9112, -0.3340,  0.1679,  0.3911,    -inf,    -inf],\n",
              "        [-0.9563, -0.5518, -0.0837,  0.4482,  0.9669, -0.7583, -1.1009, -0.1881,\n",
              "          1.0936, -0.2113,  0.1203,  0.9837, -0.4094, -0.0919,    -inf],\n",
              "        [ 0.5489,  1.1825,  1.7836, -0.7314, -0.7873,  0.6787,  0.8364,  0.4116,\n",
              "         -2.1631,  0.5310,  1.1025, -1.5901, -0.2338, -1.6032,  0.7842]])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    }
  ]
}